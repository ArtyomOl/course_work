import json
import os
import requests
import time
from text_preprocess import preprocess_text
from indexer import upload_df  # —Ç–≤–æ—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞

INDEX_PATH = r'D:\Python projects\COURSE_WORK\core\index\index.json'
WIKI_API = "https://ru.wikipedia.org/w/api.php"

# –ó–∞–≥–æ–ª–æ–≤–æ–∫, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª—É—á–∏—Ç—å 403
HEADERS = {
    "User-Agent": "WikiIndexer/1.0 (your_email@example.com)"
}


def get_random_wikipedia_article():
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é —Å—Ç–∞—Ç—å—é (—Ç–µ–∫—Å—Ç) —Å –í–∏–∫–∏–ø–µ–¥–∏–∏.
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç MediaWiki API ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω—ã–π –∏ –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è.
    """
    try:
        params = {
            "action": "query",
            "format": "json",
            "generator": "random",
            "grnnamespace": 0,  # —Ç–æ–ª—å–∫–æ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
            "prop": "extracts",
            "explaintext": True,
            "grnlimit": 1
        }
        response = requests.get(WIKI_API, params=params, headers=HEADERS, timeout=10)
        if response.status_code != 200:
            print(f"[–û—à–∏–±–∫–∞ {response.status_code}] {response.text[:200]}")
            return None

        data = response.json()
        if "query" not in data:
            return None

        page = next(iter(data["query"]["pages"].values()))
        return page.get("extract", "")
    except Exception as e:
        print(f"[–û—à–∏–±–∫–∞] {e}")
        return None


def ensure_index_exists():
    if not os.path.exists(INDEX_PATH):
        os.makedirs(os.path.dirname(INDEX_PATH), exist_ok=True)
        with open(INDEX_PATH, 'w', encoding='utf-8') as f:
            json.dump({"count": 0, "frequency": {}}, f, ensure_ascii=False, indent=4)


def increment_doc_count():
    with open(INDEX_PATH, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    meta['count'] = meta.get('count', 0) + 1
    with open(INDEX_PATH, 'w', encoding='utf-8') as f:
        json.dump(meta, f, ensure_ascii=False, indent=4)


def train_index_on_random_wiki_articles(num_articles=500):
    ensure_index_exists()
    processed = 0

    print(f"üìò –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –Ω–∞ {num_articles} —Å–ª—É—á–∞–π–Ω—ã—Ö —Å—Ç–∞—Ç—å—è—Ö –í–∏–∫–∏–ø–µ–¥–∏–∏...\n")

    for i in range(num_articles):
        raw_text = get_random_wikipedia_article()
        if not raw_text:
            time.sleep(1)
            continue

        clean_text = preprocess_text(raw_text)
        words = clean_text.split()

        if len(words) < 10:
            continue

        upload_df(words)
        increment_doc_count()
        processed += 1

        if (i + 1) % 20 == 0:
            print(f"‚Üí –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed} —Å—Ç–∞—Ç–µ–π")

        # –ù–µ —Å–ø–µ—à–∏–º, —á—Ç–æ–±—ã –Ω–µ –ø–æ–ª—É—á–∏—Ç—å 429 –∏–ª–∏ 403
        time.sleep(1)

    print(f"\n‚úÖ –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –í—Å–µ–≥–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {processed} —Å—Ç–∞—Ç–µ–π.")

    with open(INDEX_PATH, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    print(f"üì¶ –í –∏–Ω–¥–µ–∫—Å–µ {len(meta['frequency'])} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤ –∏–∑ {meta['count']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.")


if __name__ == "__main__":
    train_index_on_random_wiki_articles(num_articles=1000)
